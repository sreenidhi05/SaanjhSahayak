{"ast":null,"code":"// // import { HfInference } from \"npm:@huggingface/inference\"\n// import { HfInference } from '@huggingface/inference'\n// import { InferenceAPIClient } from '@huggingface/inference';\n\n// const hf = new HfInference('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')\n\n// // Create a client instance\n// const client = new InferenceAPIClient({ apiKey: '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' });\n\n// // Function to generate text using the BioMistral 7B model\n// export const generateText = async (prompt, maxTokens = 50) => {\n//   try {\n//     const response = await client.generate({\n//       modelId: 'mdpeterson/biomistral-7b',\n//       prompt,\n//       maxTokens,\n//     });\n\n//     return response.data;\n//   } catch (error) {\n//     throw new Error('Error generating text:', error);\n//   }\n// };\n\n// import { HfInference } from \"npm:@huggingface/inference\"\nimport { HfInference } from '@huggingface/inference';\n\n// Initialize HfInference\nconst hf = new HfInference('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');\n\n// Function to generate text using the BioMistral 7B model\nexport const generateText = async (prompt, maxTokens = 50) => {\n  try {\n    // Use hf instance to generate text\n    const response = await hf.generate({\n      modelId: 'mdpeterson/biomistral-7b',\n      prompt,\n      maxTokens\n    });\n    return response.data;\n  } catch (error) {\n    throw new Error('Error generating text:', error);\n  }\n};","map":{"version":3,"names":["HfInference","hf","generateText","prompt","maxTokens","response","generate","modelId","data","error","Error"],"sources":["E:/SaanjhSahaayak/temp/tempo/src/api/services/huggingfaceServices.js"],"sourcesContent":["// // import { HfInference } from \"npm:@huggingface/inference\"\r\n// import { HfInference } from '@huggingface/inference'\r\n// import { InferenceAPIClient } from '@huggingface/inference';\r\n\r\n\r\n\r\n// const hf = new HfInference('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')\r\n\r\n\r\n// // Create a client instance\r\n// const client = new InferenceAPIClient({ apiKey: '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' });\r\n\r\n// // Function to generate text using the BioMistral 7B model\r\n// export const generateText = async (prompt, maxTokens = 50) => {\r\n//   try {\r\n//     const response = await client.generate({\r\n//       modelId: 'mdpeterson/biomistral-7b',\r\n//       prompt,\r\n//       maxTokens,\r\n//     });\r\n\r\n//     return response.data;\r\n//   } catch (error) {\r\n//     throw new Error('Error generating text:', error);\r\n//   }\r\n// };\r\n\r\n\r\n// import { HfInference } from \"npm:@huggingface/inference\"\r\nimport { HfInference } from '@huggingface/inference'\r\n\r\n// Initialize HfInference\r\nconst hf = new HfInference('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')\r\n\r\n// Function to generate text using the BioMistral 7B model\r\nexport const generateText = async (prompt, maxTokens = 50) => {\r\n  try {\r\n    // Use hf instance to generate text\r\n    const response = await hf.generate({\r\n      modelId: 'mdpeterson/biomistral-7b',\r\n      prompt,\r\n      maxTokens,\r\n    });\r\n\r\n    return response.data;\r\n  } catch (error) {\r\n    throw new Error('Error generating text:', error);\r\n  }\r\n};\r\n"],"mappings":"AAAA;AACA;AACA;;AAIA;;AAGA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAGA;AACA,SAASA,WAAW,QAAQ,wBAAwB;;AAEpD;AACA,MAAMC,EAAE,GAAG,IAAID,WAAW,CAAC,uCAAuC,CAAC;;AAEnE;AACA,OAAO,MAAME,YAAY,GAAG,MAAAA,CAAOC,MAAM,EAAEC,SAAS,GAAG,EAAE,KAAK;EAC5D,IAAI;IACF;IACA,MAAMC,QAAQ,GAAG,MAAMJ,EAAE,CAACK,QAAQ,CAAC;MACjCC,OAAO,EAAE,0BAA0B;MACnCJ,MAAM;MACNC;IACF,CAAC,CAAC;IAEF,OAAOC,QAAQ,CAACG,IAAI;EACtB,CAAC,CAAC,OAAOC,KAAK,EAAE;IACd,MAAM,IAAIC,KAAK,CAAC,wBAAwB,EAAED,KAAK,CAAC;EAClD;AACF,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}