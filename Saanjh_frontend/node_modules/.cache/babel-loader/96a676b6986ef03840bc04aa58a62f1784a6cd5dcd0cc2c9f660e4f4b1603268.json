{"ast":null,"code":"\n// // import { HfInference } from \"npm:@huggingface/inference\"\n// import { HfInference } from '@huggingface/inference'\n\n// // Initialize HfInference\n// const hf = new HfInference('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')\n\n// // Function to generate text using the BioMistral 7B model\n// export const generateText = async (prompt, maxTokens = 50) => {\n//   try {\n//     // Use hf instance to generate text\n//     const response = await hf.generate({\n//       modelId: 'BioMistral/BioMistral-7B',\n//       prompt,\n//       maxTokens,\n//     });\n\n//     return response.data;\n//   } catch (error) {\n//     throw new Error('Error generating text:', error);\n//   }\n// };","map":{"version":3,"names":[],"sources":["E:/TestSaanjhSahayak/temp/tempo/src/api/services/huggingfaceServices.js"],"sourcesContent":["\r\n// // import { HfInference } from \"npm:@huggingface/inference\"\r\n// import { HfInference } from '@huggingface/inference'\r\n\r\n// // Initialize HfInference\r\n// const hf = new HfInference('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')\r\n\r\n// // Function to generate text using the BioMistral 7B model\r\n// export const generateText = async (prompt, maxTokens = 50) => {\r\n//   try {\r\n//     // Use hf instance to generate text\r\n//     const response = await hf.generate({\r\n//       modelId: 'BioMistral/BioMistral-7B',\r\n//       prompt,\r\n//       maxTokens,\r\n//     });\r\n\r\n//     return response.data;\r\n//   } catch (error) {\r\n//     throw new Error('Error generating text:', error);\r\n//   }\r\n// };\r\n"],"mappings":";AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}